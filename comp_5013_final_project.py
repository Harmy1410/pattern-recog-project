# -*- coding: utf-8 -*-
"""COMP 5013 - Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vGfh9WScnd22DtFlyhJOag3KsgkG4rc1
"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score
from os import listdir
from os.path import exists
from keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten, Dense, Conv2DTranspose
from keras.models import Model, load_model
from keras.callbacks import EarlyStopping
from matplotlib import pyplot as plt
from google.cloud import storage

path = 'full/numpy_bitmap/'
# We choose the 5 best performing classes from the paper.
classes = ['airplane', 'car', 'bird', 'sailboat', 'truck']
client = storage.Client.create_anonymous_client()
bucket = client.bucket(bucket_name='quickdraw_dataset', user_project=None)
for label in classes:
    temp_path = path + label +'.npy'
    blob = bucket.blob(temp_path)
    blob.download_to_filename(label + '.npy')

def load_data(root: str = '', int_label=True) -> tuple:
    labels = []
    data = None
    files = [dir for dir in listdir(root) if '.npy' in dir]
    i = 0
    for dir in files:
      label = dir.split('.')[0]
      temp_data = np.load(root+dir) 
      hot_encoded_label = [0] * len(files)
      hot_encoded_label[i] = 1
      labels += [hot_encoded_label] * temp_data.shape[0] if int_label else [label] * temp_data.shape[0]
      data = temp_data if data is None else np.append(data, temp_data, 0)
      i+=1
    # The original data are 28, 28 grayscale bitmaps
    return data.reshape(-1,28,28,1).astype('float') / np.max(data), labels

def get_autoencoder(train_data, test_data) -> tuple:
    if exists('/content/autoencoder'):
        model = load_model('/content/autoencoder')
        encoder = Model(model.input, model.layers[2].output)
        decoder_input = Input(shape=(7,7,3))
        decoder_layer1 = model.layers[-2](decoder_input)
        decoder = Model(decoder_input, model.layers[-1](decoder_layer1))
        return encoder, decoder
    
    input = Input(shape=(28, 28, 1))
    layer = Conv2D(32, (4, 4), activation='relu', strides=(2, 2), padding='same')(input)
    layer = Conv2D(3, (2, 2), activation='sigmoid', strides=(2,2), padding='same')(layer)
    layer = Conv2DTranspose(32, (2, 2), activation='relu', strides=(2,2), padding='same')(layer)
    layer = Conv2DTranspose(1, (4, 4), activation='sigmoid', strides=(2,2), padding='same')(layer)
    model = Model(input, layer)
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy')

    callback = EarlyStopping(monitor='loss', patience=3)
    model.fit(train_data, train_data, epochs = 50, batch_size = 128, shuffle = True, validation_data = (test_data, test_data), callbacks=[callback])
    model.save('/content/autoencoder')

    encoder = Model(model.input, model.layers[2].output)
    decoder_input = Input(shape=(7,7,3))
    decoder_layer1 = model.layers[-2](decoder_input)
    decoder = Model(decoder_input, model.layers[-1](decoder_layer1))
    return encoder, decoder

def display(data, labels):
    labels_array = np.array(labels)
    classes = np.unique(labels_array)

    plt.figure(figsize=(20, len(classes)))
    for i in range(len(classes)):
        ax = plt.subplot(2, len(classes), i+1)
        plt.imshow(data[labels_array == classes[i]][-1].reshape(28,28))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()
  
def get_model1(train_data, test_data, train_labels, test_labels) -> Model:
    if exists('/content/model1'):
        return load_model('/content/model1')
    
    input = Input(shape=(7, 7, 3))
    layer = Conv2D(16, (3, 3), activation='relu', padding='same')(input)
    layer = Conv2D(32, (3, 3), activation='relu', padding='same')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Conv2D(64, (3, 3), activation='relu', padding='same', strides=(1, 1))(layer)
    layer = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(1, 1))(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Flatten()(layer)
    layer = Dense(512, activation='relu')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Dense(32, activation='relu')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Dense(5, activation='softmax')(layer)

    model = Model(input, layer)
    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
    callback = EarlyStopping(monitor='loss', patience=3)
    model.fit(train_data, train_labels, epochs = 50, batch_size = 128, shuffle = True, validation_data = (test_data, test_labels), callbacks=[callback])
    model.save('/content/model1')
    return model
  
def get_simplified_model1(train_data, test_data, train_labels, test_labels) -> Model:
    if exists('/content/simplified_model1'):
        return load_model('/content/simplified_model1')
    
    input = Input(shape=(7, 7, 3))
    layer = Conv2D(16, (3, 3), activation='relu', padding='same')(input)
    layer = Conv2D(32, (3, 3), activation='relu', padding='same')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Flatten()(layer)
    layer = Dense(512, activation='relu')(layer)
    layer = Dense(32, activation='relu')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Dense(5, activation='softmax')(layer)
    model = Model(input, layer)

    model = Model(input, layer)
    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
    callback = EarlyStopping(monitor='loss', patience=3)
    model.fit(train_data, train_labels, epochs = 50, batch_size = 128, shuffle = True, validation_data = (test_data, test_labels), callbacks=[callback])
    model.save('/content/simplified_model1')
    return model

train_data, test_data, train_labels, test_labels = train_test_split(*load_data('/content/'), test_size=0.2)
encoder, decoder = get_autoencoder(train_data, test_data)

train_data, valid_data, train_labels, valid_labels = train_test_split(train_data, train_labels, test_size=0.2)
encoded_train_data = encoder.predict(train_data)
encoded_test_data = encoder.predict(test_data)
encoded_valid_data = encoder.predict(valid_data)

model1 = get_model1(encoded_train_data, encoded_valid_data, np.array(train_labels), np.array(valid_labels))

simplified_model1 = get_simplified_model1(encoded_train_data, encoded_valid_data, np.array(train_labels), np.array(valid_labels))

char_labels = ['car', 'bird', 'airplane', 'truck', 'ship']

char_test_labels = []
for label in test_labels:
  char_test_labels.append(char_labels[label.index(1)])

model_predictions = model1.predict(encoded_test_data)
char_model_predictions = []

for label in model_predictions:
  char_model_predictions.append(char_labels[np.argmax(label)])

print(accuracy_score(char_test_labels, char_model_predictions))
print(roc_auc_score(np.array(test_labels), model_predictions))
confusion_matrix(char_test_labels, char_model_predictions)

model_predictions = simplified_model1.predict(encoded_test_data)
char_model_predictions = []

for label in model_predictions:
  char_model_predictions.append(char_labels[np.argmax(label)])

print(accuracy_score(char_test_labels, char_model_predictions))
print(roc_auc_score(np.array(test_labels), model_predictions))
confusion_matrix(char_test_labels, char_model_predictions)