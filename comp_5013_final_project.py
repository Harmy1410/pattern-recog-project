# -*- coding: utf-8 -*-
"""COMP 5013 - Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vGfh9WScnd22DtFlyhJOag3KsgkG4rc1
"""

__author__ = 'Asif Rasheed'

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score
from os import listdir
from os.path import exists
from keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten, Dense, Conv2DTranspose
from keras.models import Model, load_model
from keras.callbacks import EarlyStopping
from matplotlib import pyplot as plt
from google.cloud import storage

path = 'full/numpy_bitmap/' # path of the preprocessed data in the bucket
# We choose the 5 best performing classes from the literature. 
classes = ['airplane', 'car', 'bird', 'sailboat', 'truck']
client = storage.Client.create_anonymous_client()
bucket = client.bucket(bucket_name='quickdraw_dataset', user_project=None)
# Downloads the classes to files
for label in classes:
    temp_path = path + label +'.npy'
    blob = bucket.blob(temp_path)
    blob.download_to_filename(label + '.npy')

def load_data(root: str = './', int_label: bool = True) -> tuple:
    """Function to load image data from given directory.

    If the int_label flag is set to true, the label is one hot
    encoded else the label is a string. The data is loaded
    from the files already downloaded from the google bucket.

    Args:
        root: The directory containing the data files.
        int_label: Flag to enable or disable one hot encoding the label.
    Returns:
        The normalized data and the labels
    """
    labels = []
    data = None
    files = [dir for dir in listdir(root) if '.npy' in dir]
    i = 0
    for dir in files:
      label = dir.split('.')[0]
      temp_data = np.load(root+dir) 
      hot_encoded_label = [0] * len(files)
      hot_encoded_label[i] = 1
      labels += [hot_encoded_label] * temp_data.shape[0] if int_label else [label] * temp_data.shape[0]
      data = temp_data if data is None else np.append(data, temp_data, 0)
      i+=1
    # The original data are 28, 28 grayscale bitmaps
    return data.reshape(-1,28,28,1).astype('float') / np.max(data), labels

def get_autoencoder(train_data, test_data) -> tuple:
    """Function to generate autoencoder using the given data.

    If the autoencoder was already trained, the function would
    load the saved autoencoder model from file, extract the 
    encoder and decoder and returns them. Else will compile
    a new model and train it using the given data, save it to 
    file and returns the encoder and decoder separately. 
    
    Args:
        train_data: The data used to train the autoencoder.
        test_data: The data used to validate the autoencoder.
    Returns:
        The enoder and the decoder.
    """
    if exists('/content/autoencoder'):
        model = load_model('/content/autoencoder')
        encoder = Model(model.input, model.layers[2].output)
        decoder_input = Input(shape=(7,7,3))
        decoder_layer1 = model.layers[-2](decoder_input)
        decoder = Model(decoder_input, model.layers[-1](decoder_layer1))
        return encoder, decoder
    
    input = Input(shape=(28, 28, 1))
    layer = Conv2D(32, (4, 4), activation='relu', strides=(2, 2), padding='same')(input)
    layer = Conv2D(3, (2, 2), activation='sigmoid', strides=(2,2), padding='same')(layer)
    layer = Conv2DTranspose(32, (2, 2), activation='relu', strides=(2,2), padding='same')(layer)
    layer = Conv2DTranspose(1, (4, 4), activation='sigmoid', strides=(2,2), padding='same')(layer)
    model = Model(input, layer)
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy')

    callback = EarlyStopping(monitor='loss', patience=3)
    model.fit(train_data, train_data, epochs = 50, batch_size = 128, shuffle = True, validation_data = (test_data, test_data), callbacks=[callback])
    model.save('/content/autoencoder')

    encoder = Model(model.input, model.layers[2].output)
    decoder_input = Input(shape=(7,7,3))
    decoder_layer1 = model.layers[-2](decoder_input)
    decoder = Model(decoder_input, model.layers[-1](decoder_layer1))
    return encoder, decoder

def display(data, labels):
    """Function to display samples from given dataset.
    
    The function will print one sample from each class
    in the given dataset.
    
    Args:
        data: The image data.
        labels: The labels associated with it.
    """
    labels_array = np.array(labels)
    classes = np.unique(labels_array)

    plt.figure(figsize=(20, len(classes)))
    for i in range(len(classes)):
        ax = plt.subplot(2, len(classes), i+1)
        plt.imshow(data[labels_array == classes[i]][-1].reshape(28,28))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()
  
def get_model1(train_data, test_data, train_labels, test_labels) -> Model:
    """Function to generate the original model from literature.
    
    The function, if the model was already trained, will load the model
    from file and return it. Else it will compile the model and train
    it using the data provided. There is an early stopping condition
    with patience of 3 which will stop the training if the loss doesn't
    decrease for 3 epochs. This is the modified version of the model
    from the literature which could accept the encoded data.
    
    Args:
        train_data: The encoded data used for training.
        test_data: The encoded data used for validation.
        train_labels: The one hot encoded labels associated with the train_data.
        test_data: The one hot encoded labels associated with the test_data.
    Returns:
        The trained model from literature.
    """
    if exists('/content/model1'):
        return load_model('/content/model1')
    
    input = Input(shape=(7, 7, 3))
    layer = Conv2D(16, (3, 3), activation='relu', padding='same')(input)
    layer = Conv2D(32, (3, 3), activation='relu', padding='same')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Conv2D(64, (3, 3), activation='relu', padding='same', strides=(1, 1))(layer)
    layer = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(1, 1))(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Flatten()(layer)
    layer = Dense(512, activation='relu')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Dense(32, activation='relu')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Dense(5, activation='softmax')(layer)

    model = Model(input, layer)
    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
    callback = EarlyStopping(monitor='loss', patience=3)
    model.fit(train_data, train_labels, epochs = 50, batch_size = 128, shuffle = True, validation_data = (test_data, test_labels), callbacks=[callback])
    model.save('/content/model1')
    return model
  
def get_simplified_model1(train_data, test_data, train_labels, test_labels) -> Model:
    """Function to generate the proposed simplified model.
    
    The function, if the model was already trained, will load the model
    from file and return it. Else it will compile the model and train
    it using the data provided. There is an early stopping condition
    with patience of 3 which will stop the training if the loss doesn't
    decrease for 3 epochs. This is the simplified model proposed in the 
    paper.
    
    Args:
        train_data: The encoded data used for training.
        test_data: The encoded data used for validation.
        train_labels: The one hot encoded labels associated with the train_data.
        test_data: The one hot encoded labels associated with the test_data.
    Returns:
        The trained model.
    """
    if exists('/content/simplified_model1'):
        return load_model('/content/simplified_model1')
    
    input = Input(shape=(7, 7, 3))
    layer = Conv2D(16, (3, 3), activation='relu', padding='same')(input)
    layer = Conv2D(32, (3, 3), activation='relu', padding='same')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Flatten()(layer)
    layer = Dense(512, activation='relu')(layer)
    layer = Dense(32, activation='relu')(layer)
    layer = BatchNormalization(momentum=0.99)(layer)
    layer = Dropout(rate=0.4)(layer)
    layer = Dense(5, activation='softmax')(layer)
    model = Model(input, layer)

    model = Model(input, layer)
    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
    callback = EarlyStopping(monitor='loss', patience=3)
    model.fit(train_data, train_labels, epochs = 50, batch_size = 128, shuffle = True, validation_data = (test_data, test_labels), callbacks=[callback])
    model.save('/content/simplified_model1')
    return model

# Split data into training and testing sets with 80% data used for training and 20% for testing
train_data, test_data, train_labels, test_labels = train_test_split(*load_data('/content/'), test_size=0.2)
# Gets the autoencoder trained using the train data
encoder, decoder = get_autoencoder(train_data, test_data)

# Split the train data in to training and validation data with 80% data used for training and 20% for validation
# In effect, 60% will be used for actual traning and 20% for validation and testing each
train_data, valid_data, train_labels, valid_labels = train_test_split(train_data, train_labels, test_size=0.2)
# Encodes the data
encoded_train_data = encoder.predict(train_data)
encoded_test_data = encoder.predict(test_data)
encoded_valid_data = encoder.predict(valid_data)

# Train the original model proposed in the literature using the encoded training data
model1 = get_model1(encoded_train_data, encoded_valid_data, np.array(train_labels), np.array(valid_labels))

# Train the proposed model using the encoded training data
simplified_model1 = get_simplified_model1(encoded_train_data, encoded_valid_data, np.array(train_labels), np.array(valid_labels))

char_labels = ['car', 'bird', 'airplane', 'truck', 'ship']

# Converts the one-hot encoded labels into string labels
char_test_labels = []
for label in test_labels:
  char_test_labels.append(char_labels[label.index(1)])

# Generate predictions using test data
model_predictions = model1.predict(encoded_test_data)
char_model_predictions = []

# Convert the prediction into string 
for label in model_predictions:
  char_model_predictions.append(char_labels[np.argmax(label)])

# Generates the accuracy, AUC or ROC curve and Confusion Matrix for the model
print(accuracy_score(char_test_labels, char_model_predictions))
print(roc_auc_score(np.array(test_labels), model_predictions))
confusion_matrix(char_test_labels, char_model_predictions)

# Generate predictions using test data
model_predictions = simplified_model1.predict(encoded_test_data)
char_model_predictions = []

# Convert the prediction into string 
for label in model_predictions:
  char_model_predictions.append(char_labels[np.argmax(label)])

# Generates the accuracy, AUC or ROC curve and Confusion Matrix for the model
print(accuracy_score(char_test_labels, char_model_predictions))
print(roc_auc_score(np.array(test_labels), model_predictions))
confusion_matrix(char_test_labels, char_model_predictions)